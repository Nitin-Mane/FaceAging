{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the first notebook of the project. It trains a bare-minimum network for face aging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_EJlmH-bj-_W",
    "outputId": "2dddf11f-9b6a-4be5-c37d-a93e4b02f8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPblHQXSAwdA"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/My Drive/UTK_inthewild/part1.tar.gz' .\n",
    "!cp '/content/drive/My Drive/UTK_inthewild/part2.tar.gz' .\n",
    "!cp '/content/drive/My Drive/UTK_inthewild/part3.tar.gz' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-KupcYhBTL8"
   },
   "outputs": [],
   "source": [
    "!mkdir images\n",
    "!tar xzf part1.tar.gz -C images\n",
    "!tar xzf part2.tar.gz -C images\n",
    "!tar xzf part3.tar.gz -C images\n",
    "!mv images/part1/* images\n",
    "!mv images/part2/* images\n",
    "!mv images/part3/* images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8KZ3up-FSdy"
   },
   "outputs": [],
   "source": [
    "!rm -r images/part1\n",
    "!rm -r images/part2\n",
    "!rm -r images/part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RGiQsdcIF8n-",
    "outputId": "15cbaef8-e55e-479c-ab35-ea4bdb549c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24109\n"
     ]
    }
   ],
   "source": [
    "!ls -l images | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cs5dzQPLh3BW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PLK9Zfkijhh"
   },
   "outputs": [],
   "source": [
    "class InstanceNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        depth = input_shape[3]\n",
    "        self.scale = self.add_weight(name=\"scale_\"+str(depth),\n",
    "                                     shape=[depth], \n",
    "                                     initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32),\n",
    "                                     dtype=tf.float32\n",
    "                                    )\n",
    "            \n",
    "        self.offset = self.add_weight(name=\"offset_\"+str(depth),\n",
    "                                      shape=[depth], \n",
    "                                      initializer=tf.constant_initializer(0.0),\n",
    "                                      dtype=tf.float32\n",
    "                                    )\n",
    "        super(InstanceNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, input_):\n",
    "        mean, variance = tf.nn.moments(input_, axes=[1,2], keep_dims=True)\n",
    "        epsilon = 1e-5\n",
    "        inv = tf.rsqrt(variance + epsilon)\n",
    "        normalized = (input_-mean)*inv\n",
    "        return self.scale*normalized + self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "om8YGrKnYDW0"
   },
   "outputs": [],
   "source": [
    "class Residual(Layer):\n",
    "    def __init__(self, dim=256, ks=3, s=1, padding='VALID', stddev=0.02, from_list=None, **kwargs):\n",
    "        self.dim = dim\n",
    "        self.ks = ks\n",
    "        self.s = s\n",
    "        self.from_list = from_list\n",
    "        self.padding=padding\n",
    "        self.stddev=stddev\n",
    "        super(Residual, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv2d1 = Conv2D(filters=self.dim, kernel_size=self.ks, strides=self.s, padding=self.padding, activation=None,\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=self.stddev),\n",
    "                            bias_initializer=None)\n",
    "        self.instnorm1 = InstanceNormalization()\n",
    "        self.relu1 = ReLU()\n",
    "        self.conv2d2 = Conv2D(filters=self.dim, kernel_size=self.ks, strides=self.s, padding=self.padding, activation=None,\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=self.stddev),\n",
    "                            bias_initializer=None)\n",
    "        self.instnorm2 = InstanceNormalization()\n",
    "        \n",
    "        super(Residual, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        p = int((self.ks - 1) / 2)\n",
    "        y = tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]], \"REFLECT\")\n",
    "\n",
    "        y = self.instnorm1(self.conv2d1(y))\n",
    "        y = tf.pad(self.relu1(y), [[0, 0], [p, p], [p, p], [0, 0]], \"REFLECT\")\n",
    "        y = self.instnorm2(self.conv2d2(y))\n",
    "        return y + x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZD54eEauV0g"
   },
   "outputs": [],
   "source": [
    "def instance_norm(inp, name=\"instance_norm\"):\n",
    "    return InstanceNormalization()(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvvZThB7tci3"
   },
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding='SAME', name=\"conv2d\"):\n",
    "    return Conv2D(filters=output_dim, kernel_size=ks, strides=s, padding=padding, activation=None,\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "                            bias_initializer=None)(input_)\n",
    "\n",
    "def deconv2d(input_, output_dim, ks=4, s=2, stddev=0.02, name=\"deconv2d\"):\n",
    "    return Conv2DTranspose(filters=output_dim, kernel_size=ks, strides=s, padding='SAME', activation=None,\n",
    "                                    kernel_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "                                    bias_initializer=None)(input_)\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return LeakyReLU(leak)(x)\n",
    "\n",
    "@tf.function\n",
    "def abs_criterion(in_, target):\n",
    "    return tf.reduce_mean(tf.abs(in_ - target))\n",
    "\n",
    "@tf.function\n",
    "def mae_criterion(in_, target):\n",
    "    return tf.reduce_mean((in_-target)**2)\n",
    "\n",
    "@tf.function\n",
    "def sce_criterion(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgXMBAVZWVQS"
   },
   "outputs": [],
   "source": [
    "for d in os.listdir('.'):\n",
    "    if d[0].isdigit():\n",
    "        os.system('rm -r ' + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ght9KYlbu7DC"
   },
   "outputs": [],
   "source": [
    "def make_discriminator(name=\"discriminator\"):\n",
    "\n",
    "    inp = Input((128, 128, 3))\n",
    "    df_dim = 64\n",
    "\n",
    "    h0 = lrelu(conv2d(inp, df_dim, name='d_h0_conv'))\n",
    "    # h0 is (128 x 128 x self.df_dim)\n",
    "    h1 = lrelu(instance_norm(conv2d(h0, df_dim*2, name='d_h1_conv'), 'd_bn1'))\n",
    "    # h1 is (64 x 64 x self.df_dim*2)\n",
    "    h2 = lrelu(instance_norm(conv2d(h1, df_dim*4, name='d_h2_conv'), 'd_bn2'))\n",
    "    # h2 is (32x 32 x self.df_dim*4)\n",
    "    h3 = lrelu(instance_norm(conv2d(h2, df_dim*8, s=1, name='d_h3_conv'), 'd_bn3'))\n",
    "    # h3 is (32 x 32 x self.df_dim*8)\n",
    "    h4 = conv2d(h3, 1, s=1, name='d_h3_pred')\n",
    "    # h4 is (32 x 32 x 1)\n",
    "    m = Model(inputs=[inp], outputs=[h4])\n",
    "    return m\n",
    "\n",
    "def make_generator_resnet(name=\"generator\"):\n",
    "\n",
    "    inp = Input((128, 128, 3))\n",
    "    gf_dim = 64\n",
    "    output_c_dim = 3\n",
    "\n",
    "    c0 = tf.pad(inp, [[0, 0], [3, 3], [3, 3], [0, 0]], \"REFLECT\")\n",
    "    c1 = ReLU()(instance_norm(conv2d(c0, gf_dim, 7, 1, padding='VALID', name='g_e1_c'), 'g_e1_bn'))\n",
    "    c2 = ReLU()(instance_norm(conv2d(c1, gf_dim*2, 3, 2, name='g_e2_c'), 'g_e2_bn'))\n",
    "    c3 = ReLU()(instance_norm(conv2d(c2, gf_dim*4, 3, 2, name='g_e3_c'), 'g_e3_bn'))        # 64 *64\n",
    "    \n",
    "    r1 = Residual(dim=gf_dim*4, name='g_r1')(c3)\n",
    "    r2 = Residual(dim=gf_dim*4, name='g_r2')(r1)\n",
    "    r3 = Residual(dim=gf_dim*4, name='g_r3')(r2)\n",
    "    r4 = Residual(dim=gf_dim*4, name='g_r4')(r3)\n",
    "    r5 = Residual(dim=gf_dim*4, name='g_r5')(r4)\n",
    "    r6 = Residual(dim=gf_dim*4, name='g_r6')(r5)\n",
    "    r7 = Residual(dim=gf_dim*4, name='g_r7')(r6)\n",
    "    r8 = Residual(dim=gf_dim*4, name='g_r8')(r7)\n",
    "    r9 = Residual(dim=gf_dim*4, name='g_r9')(r8)\n",
    "\n",
    "    d1 = deconv2d(r9, gf_dim*2, 3, 2, name='g_d1_dc')\n",
    "    d1 = ReLU()(instance_norm(d1, 'g_d1_bn'))\n",
    "    d2 = deconv2d(d1, gf_dim, 3, 2, name='g_d2_dc')\n",
    "    d2 = ReLU()(instance_norm(d2, 'g_d2_bn'))\n",
    "    d2 = tf.pad(d2, [[0, 0], [3, 3], [3, 3], [0, 0]], \"REFLECT\")\n",
    "\n",
    "    pred = Activation(\"tanh\")(conv2d(d2, output_c_dim, 7, 1, padding='VALID', name='g_pred_c'))\n",
    "    m = Model(inputs=[inp], outputs=[pred])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "8yoRICCsuvVx",
    "outputId": "04bfdbe0-3028-45e5-a0f4-82a0a797b9bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 10:09:41.283514 139718129379200 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model_D\n",
      "Loading model_E\n",
      "Loading model_G\n",
      "Loading model_H\n"
     ]
    }
   ],
   "source": [
    "model_G = make_generator_resnet(\"G\")                   # First Generator\n",
    "model_H = make_generator_resnet(\"H\")                   # Second Generator\n",
    "model_D = make_discriminator(\"D\")                   # Discriminator for G\n",
    "model_E = make_discriminator(\"E\")                   # Discriminator for H\n",
    "\n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/model_D_age.h5'):\n",
    "    print(\"Loading model_D\")\n",
    "    model_D.load_weights('/content/drive/My Drive/UTK_inthewild/model_D_age.h5')\n",
    "    \n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/model_E_age.h5'):\n",
    "    print(\"Loading model_E\")\n",
    "    model_E.load_weights('/content/drive/My Drive/UTK_inthewild/model_E_age.h5')\n",
    "    \n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/model_G_age.h5'):\n",
    "    print(\"Loading model_G\")\n",
    "    model_G.load_weights('/content/drive/My Drive/UTK_inthewild/model_G_age.h5')\n",
    "    \n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/model_H_age.h5'):\n",
    "    print(\"Loading model_H\")\n",
    "    model_H.load_weights('/content/drive/My Drive/UTK_inthewild/model_H_age.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_jeGxEI9o7Mh",
    "outputId": "64aecac2-9031-4cd7-dad7-c0a856359b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_MirrorPad_2 (Ten [(None, 134, 134, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 64)      9472      \n",
      "_________________________________________________________________\n",
      "instance_normalization_5 (In (None, 128, 128, 64)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "instance_normalization_6 (In (None, 64, 64, 128)       256       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "instance_normalization_7 (In (None, 32, 32, 256)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "residual_9 (Residual)        (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_10 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_11 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_12 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_13 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_14 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_15 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_16 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "residual_17 (Residual)       (None, 32, 32, 256)       1181184   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 128)       295040    \n",
      "_________________________________________________________________\n",
      "instance_normalization_8 (In (None, 64, 64, 128)       256       \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 128, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "instance_normalization_9 (In (None, 128, 128, 64)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_MirrorPad_3 (Ten [(None, 134, 134, 64)]    0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 3)       9411      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 3)       0         \n",
      "=================================================================\n",
      "Total params: 11,388,675\n",
      "Trainable params: 11,388,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 64, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 128)       131200    \n",
      "_________________________________________________________________\n",
      "instance_normalization_13 (I (None, 32, 32, 128)       256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 256)       524544    \n",
      "_________________________________________________________________\n",
      "instance_normalization_14 (I (None, 16, 16, 256)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "instance_normalization_15 (I (None, 16, 16, 512)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 1)         8193      \n",
      "=================================================================\n",
      "Total params: 2,766,529\n",
      "Trainable params: 2,766,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_H.summary()\n",
    "model_E.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwL_lRpEIICu"
   },
   "outputs": [],
   "source": [
    "def get_image_list(path='images', young_lower=18, young_upper=30, old_lower=50, old_upper=80):\n",
    "    images = os.listdir(path)\n",
    "    ages = {\n",
    "        'young': [],\n",
    "        'old': []\n",
    "    }\n",
    "    \n",
    "    for image in images:\n",
    "        age = int(image.split('_')[0])\n",
    "        if young_lower <= age <= young_upper:\n",
    "            ages['young'].append(image)\n",
    "        if old_upper >= age >= old_lower:\n",
    "            ages['old'].append(image)\n",
    "        \n",
    "    return ages\n",
    "\n",
    "image_dataset = get_image_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hllf6s80JI6Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def generator(gen_func, *args):\n",
    "    while True:\n",
    "        yield gen_func(*args)\n",
    "        \n",
    "def create_young_old_batch(batch_size=32, shuffle_data=True):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    x = 128\n",
    "    images = get_image_list()['young']\n",
    "    to_create = np.random.choice(images, batch_size)\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (x, x))\n",
    "        X1.append(image)\n",
    "        \n",
    "    images = get_image_list()['old']\n",
    "    to_create = np.random.choice(images, batch_size)\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (x, x))\n",
    "        X2.append(image)\n",
    "        \n",
    "    X1 = np.stack(X1).astype(np.float32)\n",
    "    X2 = np.stack(X2).astype(np.float32)\n",
    "    \n",
    "    if shuffle_data:\n",
    "        X1, X2 = shuffle(X1, X2)\n",
    "        \n",
    "    return (X1.astype(np.float32) / 127.5) - 1.0, (X2.astype(np.float32) / 127.5) - 1.0\n",
    "\n",
    "def create_young_old_test_batch(batch_size=32, shuffle_data=True):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    size = 128\n",
    "    images = get_image_list()['young']\n",
    "    to_create = images[:batch_size]\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (size, size))\n",
    "        X1.append(image)\n",
    "        \n",
    "    images = get_image_list()['old']\n",
    "    to_create = images[:batch_size]\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (size, size))\n",
    "        X2.append(image)\n",
    "        \n",
    "    X1 = np.stack(X1).astype(np.float32)\n",
    "    X2 = np.stack(X2).astype(np.float32)\n",
    "    \n",
    "    if shuffle_data:\n",
    "        X1, X2 = shuffle(X1, X2)\n",
    "        \n",
    "    return (X1.astype(np.float32) / 127.5) - 1., (X2.astype(np.float32) / 127.5) - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are the cells used for training. `GradientTape` is used instead of `model.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZa4QMp7lw5R"
   },
   "outputs": [],
   "source": [
    "g1_optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "g2_optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "d1_optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "d2_optimizer = tf.train.AdadeltaOptimizer(1.0)\n",
    "\n",
    "# disc_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# gen_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# cycled_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "identity_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = disc_loss_fn(tf.ones_like(real), real)\n",
    "    generated_loss = disc_loss_fn(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "def cycled_loss(real, cycled):\n",
    "    return cycled_loss_fn(real, cycled)\n",
    "\n",
    "def identity_loss(real, same):\n",
    "    return identity_loss_fn(real, same)\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return gen_loss_fn(tf.ones_like(generated), generated)\n",
    "\n",
    "@tf.function\n",
    "def train(genAB, genBA, discA, discB, X1, X2, lambda1=10.0):\n",
    "    real_A = X1\n",
    "    real_B = X2\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_B = genAB(real_A)                  # Young -> old\n",
    "        fake_A = genBA(real_B)                  # Old -> young\n",
    "        \n",
    "        cycled_A = genBA(fake_B)            # Young -> old -> young\n",
    "        cycled_B = genAB(fake_A)            # old -> young -> old\n",
    "        \n",
    "        same_B = genAB(real_B)                 # Old -> Old\n",
    "        same_A = genBA(real_A)                 # Young -> Young\n",
    "        \n",
    "        discA_fake_out = discA(fake_A)        # D predicts young, gen2, ie H, outputs young\n",
    "        discB_fake_out = discB(fake_B)\n",
    "\n",
    "        g_loss = mae_criterion(discA_fake_out, tf.ones_like(discA_fake_out)) \\\n",
    "                + mae_criterion(discB_fake_out, tf.ones_like(discB_fake_out)) \\\n",
    "                + lambda1 * abs_criterion(real_A, cycled_A) \\\n",
    "                + lambda1 * abs_criterion(real_B, cycled_B)  \\\n",
    "                + identity_loss(cycled_A, same_A) \\\n",
    "                + identity_loss(cycled_B, same_B)\n",
    "        \n",
    "        # cycle_loss = cycled_loss(X1, cycled_x) + cycled_loss(X2, cycled_y)\n",
    "        \n",
    "        discA_real_out = discA(real_A)\n",
    "        discB_real_out = discB(real_B)\n",
    "\n",
    "        discA_loss_real = mae_criterion(discA_real_out, tf.ones_like(discA_real_out))\n",
    "        discB_loss_real = mae_criterion(discB_real_out, tf.ones_like(discB_real_out))\n",
    "\n",
    "        discA_loss_fake = mae_criterion(discA_fake_out, tf.zeros_like(discA_fake_out))\n",
    "        discB_loss_fake = mae_criterion(discB_fake_out, tf.zeros_like(discB_fake_out))\n",
    "\n",
    "        discA_loss = (discA_loss_real + discA_loss_fake) / 2.0\n",
    "        discB_loss = (discB_loss_real + discB_loss_fake) / 2.0\n",
    "\n",
    "        d_loss = discA_loss + discB_loss\n",
    "                \n",
    "        net_loss = g_loss + d_loss\n",
    "           \n",
    "    G_vars = genAB.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, G_vars)\n",
    "    g1_optimizer.apply_gradients(zip(gradients, G_vars))\n",
    "    \n",
    "    H_vars = genBA.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, H_vars)\n",
    "    g2_optimizer.apply_gradients(zip(gradients, H_vars))\n",
    "    \n",
    "    D_vars = discA.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, D_vars)\n",
    "    d1_optimizer.apply_gradients(zip(gradients, D_vars))\n",
    "    \n",
    "    E_vars = discB.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, E_vars)\n",
    "    d2_optimizer.apply_gradients(zip(gradients, E_vars))\n",
    "    \n",
    "    del tape\n",
    "    gc.collect()\n",
    "    return net_loss, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lW3G_3DQXG4-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "Gwx6F7BQnKY1",
    "outputId": "03f9782f-0dea-4ef1-9622-cc5fe346161c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 6.8725 - generator loss: 6.1103 - discriminator loss: 0.7622Epoch  1\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 5.0532 - generator loss: 4.5698 - discriminator loss: 0.4834Epoch  2\n",
      "1341/1618 [=======================>......] - ETA: 13:10 - loss: 4.5566 - generator loss: 4.1044 - discriminator loss: 0.4522"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/optimizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# gc.collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_young_old_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         progressbar.update(i, [\n\u001b[1;32m     15\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        # X1, X2 = create_young_old_batch(batch_size=32)\n",
    "        # loss = train_discriminators(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        # gc.collect()\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/model_D.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/model_E.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/model_G.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/model_H.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8xxMyhAnACr"
   },
   "outputs": [],
   "source": [
    "model_D.save_weights('/content/drive/My Drive/UTK_inthewild/model_D_age_prev.h5')\n",
    "model_E.save_weights('/content/drive/My Drive/UTK_inthewild/model_E_age_prev.h5')\n",
    "model_G.save_weights('/content/drive/My Drive/UTK_inthewild/model_G_age_prev.h5')\n",
    "model_H.save_weights('/content/drive/My Drive/UTK_inthewild/model_H_age_prev.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOIfgTIAqGXo"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "    real_loss = disc_loss_fn(tf.ones_like(real), real)\n",
    "    generated_loss = disc_loss_fn(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "def cycled_loss(real, cycled):\n",
    "    return cycled_loss_fn(real, cycled)\n",
    "\n",
    "def identity_loss(real, same):\n",
    "    return identity_loss_fn(real, same)\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return gen_loss_fn(tf.ones_like(generated), generated)\n",
    "\n",
    "@tf.function\n",
    "def train(genAB, genBA, discA, discB, X1, X2, lambda1=10.0):\n",
    "    real_A = X1\n",
    "    real_B = X2\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_B = genAB(real_A)                  # Young -> old\n",
    "        fake_A = genBA(real_B)                  # Old -> young\n",
    "        \n",
    "        cycled_A = genBA(fake_B)            # Young -> old -> young\n",
    "        cycled_B = genAB(fake_A)            # old -> young -> old\n",
    "        \n",
    "        same_B = genAB(real_B)                 # Old -> Old\n",
    "        same_A = genBA(real_A)                 # Young -> Young\n",
    "        \n",
    "        discA_fake_out = discA(fake_A)        # D predicts young, gen2, ie H, outputs young\n",
    "        discB_fake_out = discB(fake_B)\n",
    "\n",
    "        g_loss = mae_criterion(discA_fake_out, tf.ones_like(discA_fake_out)) \\\n",
    "                + mae_criterion(discB_fake_out, tf.ones_like(discB_fake_out)) \\\n",
    "                + lambda1 * abs_criterion(real_A, cycled_A) \\\n",
    "                + lambda1 * abs_criterion(real_B, cycled_B)  # \\\n",
    "                # + identity_loss(cycled_A, same_A) \\\n",
    "                # + identity_loss(cycled_B, same_B)\n",
    "        \n",
    "        # cycle_loss = cycled_loss(X1, cycled_x) + cycled_loss(X2, cycled_y)\n",
    "        \n",
    "        discA_real_out = discA(real_A)\n",
    "        discB_real_out = discB(real_B)\n",
    "\n",
    "        discA_loss_real = mae_criterion(discA_real_out, tf.ones_like(discA_real_out))\n",
    "        discB_loss_real = mae_criterion(discB_real_out, tf.ones_like(discB_real_out))\n",
    "\n",
    "        discA_loss_fake = mae_criterion(discA_fake_out, tf.zeros_like(discA_fake_out))\n",
    "        discB_loss_fake = mae_criterion(discB_fake_out, tf.zeros_like(discB_fake_out))\n",
    "\n",
    "        discA_loss = (discA_loss_real + discA_loss_fake) / 2.0\n",
    "        discB_loss = (discB_loss_real + discB_loss_fake) / 2.0\n",
    "\n",
    "        d_loss = discA_loss + discB_loss\n",
    "                \n",
    "        net_loss = g_loss + d_loss\n",
    "           \n",
    "    G_vars = genAB.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, G_vars)\n",
    "    g1_optimizer.apply_gradients(zip(gradients, G_vars))\n",
    "    \n",
    "    H_vars = genBA.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, H_vars)\n",
    "    g2_optimizer.apply_gradients(zip(gradients, H_vars))\n",
    "    \n",
    "    D_vars = discA.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, D_vars)\n",
    "    d1_optimizer.apply_gradients(zip(gradients, D_vars))\n",
    "    \n",
    "    E_vars = discB.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, E_vars)\n",
    "    d2_optimizer.apply_gradients(zip(gradients, E_vars))\n",
    "    \n",
    "    del tape\n",
    "    gc.collect()\n",
    "    return net_loss, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "6TY7fxvUnHny",
    "outputId": "01df3978-6d92-45aa-a0d2-16a46062e967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 17:15:03.478314 139857060358016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617/1618 [============================>.] - ETA: 2s - loss: 3.4924 - generator loss: 3.0971 - discriminator loss: 0.3953Epoch  1\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 3.3495 - generator loss: 2.9561 - discriminator loss: 0.3934Epoch  2\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 3.2645 - generator loss: 2.8784 - discriminator loss: 0.3860Epoch  3\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 3.1839 - generator loss: 2.8012 - discriminator loss: 0.3827Epoch  4\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 3.1299 - generator loss: 2.7524 - discriminator loss: 0.3775Epoch  5\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 3.0886 - generator loss: 2.7175 - discriminator loss: 0.3711Epoch  6\n",
      "1475/1618 [==========================>...] - ETA: 5:16 - loss: 3.0554 - generator loss: 2.6923 - discriminator loss: 0.3631"
     ]
    }
   ],
   "source": [
    "g1_optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-5, beta_1=0.5)\n",
    "g2_optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-5, beta_1=0.5)\n",
    "d1_optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-5, beta_1=0.5)\n",
    "d2_optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-5, beta_1=0.5)\n",
    "\n",
    "gc.collect()\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        # X1, X2 = create_young_old_batch(batch_size=32)\n",
    "        # loss = train_discriminators(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        # gc.collect()\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/model_D_age.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/model_E_age.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/model_G_age.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/model_H_age.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "NKySw2RIWJy2",
    "outputId": "4c503904-dbbd-4cce-8429-1da62edf79ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 10:11:16.585262 139718129379200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617/1618 [============================>.] - ETA: 2s - loss: 2.7669 - generator loss: 2.4296 - discriminator loss: 0.3373Epoch  1\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 2.7538 - generator loss: 2.4172 - discriminator loss: 0.3366Epoch  2\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 2.7358 - generator loss: 2.3990 - discriminator loss: 0.3368Epoch  3\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 2.7327 - generator loss: 2.3957 - discriminator loss: 0.3369Epoch  4\n",
      "1617/1618 [============================>.] - ETA: 2s - loss: 2.7232 - generator loss: 2.3875 - discriminator loss: 0.3357Epoch  5\n",
      " 301/1618 [====>.........................] - ETA: 49:46 - loss: 2.7185 - generator loss: 2.3835 - discriminator loss: 0.3351"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/optimizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# gc.collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_young_old_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         progressbar.update(i, [\n\u001b[1;32m     20\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g1_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-5, beta_1=0.5)\n",
    "g2_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-5, beta_1=0.5)\n",
    "d1_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-5, beta_1=0.5)\n",
    "d2_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-5, beta_1=0.5)\n",
    "\n",
    "gc.collect()\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        # X1, X2 = create_young_old_batch(batch_size=32)\n",
    "        # loss = train_discriminators(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        # gc.collect()\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/model_D_age.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/model_E_age.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/model_G_age.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/model_H_age.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lu0qcjSlc-2B"
   },
   "outputs": [],
   "source": [
    "model_D.save_weights('/content/drive/My Drive/UTK_inthewild/model_D_age_bkp.h5')\n",
    "model_E.save_weights('/content/drive/My Drive/UTK_inthewild/model_E_age_bkp.h5')\n",
    "model_G.save_weights('/content/drive/My Drive/UTK_inthewild/model_G_age_bkp.h5')\n",
    "model_H.save_weights('/content/drive/My Drive/UTK_inthewild/model_H_age_bkp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXzbduqGj7MV"
   },
   "outputs": [],
   "source": [
    "# Evaluation cell\n",
    "\n",
    "Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "Xy = np.around((1 + Xy) * 127.5)\n",
    "Xo = np.around((1 + Xo) * 127.5)\n",
    "\n",
    "for i in range(len(Xy)):\n",
    "    cv2.imwrite(str(i)+\"_young.jpg\", Yo[i])\n",
    "    cv2.imwrite(str(i)+\"_young_orig.jpg\", Xo[i])\n",
    "    cv2.imwrite(str(i)+\"_old.jpg\", Yy[i])\n",
    "    cv2.imwrite(str(i)+\"_old_orig.jpg\", Xy[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CycleGAN_FaceAging.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
