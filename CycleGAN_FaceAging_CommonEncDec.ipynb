{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This cell takes the encoder and decoder trained previously, freezes them, and creates the generator out of them.\n",
    "\n",
    "# This is the final notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "m3orI0Iqniwx",
    "outputId": "f69f76aa-1fa7-469a-c771-0ff55b33b37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2-5jfYentI2"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/My Drive/UTK_inthewild/part1.tar.gz' .\n",
    "!cp '/content/drive/My Drive/UTK_inthewild/part2.tar.gz' .\n",
    "!cp '/content/drive/My Drive/UTK_inthewild/part3.tar.gz' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VDiCwwGnu0i"
   },
   "outputs": [],
   "source": [
    "!mkdir images\n",
    "!tar xzf part1.tar.gz -C images\n",
    "!tar xzf part2.tar.gz -C images\n",
    "!tar xzf part3.tar.gz -C images\n",
    "!mv images/part1/* images\n",
    "!mv images/part2/* images\n",
    "!mv images/part3/* images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aC2GAa-Pnw_x"
   },
   "outputs": [],
   "source": [
    "!rm -r images/part1\n",
    "!rm -r images/part2\n",
    "!rm -r images/part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QBF1_uNGnydS",
    "outputId": "abfadc0d-3a97-4822-e1f0-e73530e92e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24109\n"
     ]
    }
   ],
   "source": [
    "!ls -l images | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PH2ggg5Un0Ln"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVy2uqi0YaYN"
   },
   "outputs": [],
   "source": [
    "for d in os.listdir('.'):\n",
    "    if d[0].isdigit():\n",
    "        os.system('rm -r ' + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dy3Y5IIFn34M"
   },
   "outputs": [],
   "source": [
    "class InstanceNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        depth = input_shape[3]\n",
    "        self.scale = self.add_weight(name=\"scale_\"+str(depth),\n",
    "                                     shape=[depth], \n",
    "                                     initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32),\n",
    "                                     dtype=tf.float32\n",
    "                                    )\n",
    "            \n",
    "        self.offset = self.add_weight(name=\"offset_\"+str(depth),\n",
    "                                      shape=[depth], \n",
    "                                      initializer=tf.constant_initializer(0.0),\n",
    "                                      dtype=tf.float32\n",
    "                                    )\n",
    "        super(InstanceNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, input_):\n",
    "        mean, variance = tf.nn.moments(input_, axes=[1,2], keep_dims=True)\n",
    "        epsilon = 1e-5\n",
    "        inv = tf.rsqrt(variance + epsilon)\n",
    "        normalized = (input_-mean)*inv\n",
    "        return self.scale*normalized + self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqy4AwxTvZwz"
   },
   "outputs": [],
   "source": [
    "class Residual(Layer):\n",
    "    def __init__(self, dim=256, ks=3, s=1, padding='VALID', stddev=0.02, from_list=None, **kwargs):\n",
    "        self.dim = dim\n",
    "        self.ks = ks\n",
    "        self.s = s\n",
    "        self.from_list = from_list\n",
    "        self.padding=padding\n",
    "        self.stddev=stddev\n",
    "        super(Residual, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv2d1 = Conv2D(filters=self.dim, kernel_size=self.ks, strides=self.s, padding=self.padding, activation=None,\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=self.stddev),\n",
    "                            bias_initializer=None)\n",
    "        self.instnorm1 = InstanceNormalization()\n",
    "        self.relu1 = ReLU()\n",
    "        self.conv2d2 = Conv2D(filters=self.dim, kernel_size=self.ks, strides=self.s, padding=self.padding, activation=None,\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=self.stddev),\n",
    "                            bias_initializer=None)\n",
    "        self.instnorm2 = InstanceNormalization()\n",
    "        \n",
    "        super(Residual, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        p = int((self.ks - 1) / 2)\n",
    "        y = tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]], \"REFLECT\")\n",
    "\n",
    "        y = self.instnorm1(self.conv2d1(y))\n",
    "        y = tf.pad(self.relu1(y), [[0, 0], [p, p], [p, p], [0, 0]], \"REFLECT\")\n",
    "        y = self.instnorm2(self.conv2d2(y))\n",
    "        return y + x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFpDdoOlvezf"
   },
   "outputs": [],
   "source": [
    "def instance_norm(inp, name=\"instance_norm\"):\n",
    "    return InstanceNormalization()(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1dc6Sdhvgn-"
   },
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding='SAME', name=\"conv2d\"):\n",
    "    return Conv2D(filters=output_dim, kernel_size=ks, strides=s, padding=padding, activation=None,\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "                            bias_initializer=None)(input_)\n",
    "\n",
    "def deconv2d(input_, output_dim, ks=4, s=2, stddev=0.02, name=\"deconv2d\"):\n",
    "    return Conv2DTranspose(filters=output_dim, kernel_size=ks, strides=s, padding='SAME', activation=None,\n",
    "                                    kernel_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "                                    bias_initializer=None)(input_)\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return LeakyReLU(leak)(x)\n",
    "\n",
    "@tf.function\n",
    "def abs_criterion(in_, target):\n",
    "    return tf.reduce_mean(tf.abs(in_ - target))\n",
    "\n",
    "@tf.function\n",
    "def mae_criterion(in_, target):\n",
    "    return tf.reduce_mean((in_-target)**2)\n",
    "\n",
    "@tf.function\n",
    "def sce_criterion(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o20ROR45vhdE"
   },
   "outputs": [],
   "source": [
    "def make_discriminator(name=\"discriminator\"):\n",
    "\n",
    "    inp = Input((128, 128, 3))\n",
    "    df_dim = 64\n",
    "\n",
    "    h0 = lrelu(conv2d(inp, df_dim, name='d_h0_conv'))\n",
    "    # h0 is (128 x 128 x self.df_dim)\n",
    "    h1 = lrelu(instance_norm(conv2d(h0, df_dim*2, name='d_h1_conv'), 'd_bn1'))\n",
    "    # h1 is (64 x 64 x self.df_dim*2)\n",
    "    h2 = lrelu(instance_norm(conv2d(h1, df_dim*4, name='d_h2_conv'), 'd_bn2'))\n",
    "    # h2 is (32x 32 x self.df_dim*4)\n",
    "    h3 = lrelu(instance_norm(conv2d(h2, df_dim*8, s=1, name='d_h3_conv'), 'd_bn3'))\n",
    "    # h3 is (32 x 32 x self.df_dim*8)\n",
    "    h4 = conv2d(h3, 1, s=1, name='d_h3_pred')\n",
    "    # h4 is (32 x 32 x 1)\n",
    "    m = Model(inputs=[inp], outputs=[h4])\n",
    "    return m\n",
    "\n",
    "def make_slide(gf_dim):\n",
    "    x = Input((32, 32, 256))\n",
    "    r1 = Residual(dim=gf_dim*4, name='g_r1')(x)\n",
    "    r2 = Residual(dim=gf_dim*4, name='g_r2')(r1)\n",
    "    r3 = Residual(dim=gf_dim*4, name='g_r3')(r2)\n",
    "    r4 = Residual(dim=gf_dim*4, name='g_r4')(r3)\n",
    "    r5 = Residual(dim=gf_dim*4, name='g_r5')(r4)\n",
    "    r6 = Residual(dim=gf_dim*4, name='g_r6')(r5)\n",
    "    r7 = Residual(dim=gf_dim*4, name='g_r7')(r6)\n",
    "    r8 = Residual(dim=gf_dim*4, name='g_r8')(r7)\n",
    "    r9 = Residual(dim=gf_dim*4, name='g_r9')(r8)\n",
    "    model = Model(inputs=[x], outputs=[r9])\n",
    "    return model\n",
    "\n",
    "def make_generator_resnet(name=\"generator\"):\n",
    "\n",
    "    inp = Input((128, 128, 3))\n",
    "    gf_dim = 64\n",
    "    output_c_dim = 3\n",
    "\n",
    "    conv = tf.keras.models.load_model('/content/drive/My Drive/UTK_inthewild/conv_model.h5', {'InstanceNormalization': InstanceNormalization, 'tf': tf})\n",
    "    deconv = tf.keras.models.load_model('/content/drive/My Drive/UTK_inthewild/deconv_model.h5', {'InstanceNormalization': InstanceNormalization, 'tf': tf})\n",
    "    conv.trainable = False\n",
    "    deconv.trainable = False\n",
    "    \n",
    "    x = conv(inp)\n",
    "    res = make_slide(gf_dim)\n",
    "    res_out = res(x)\n",
    "    out = deconv(res_out)\n",
    "\n",
    "    m = Model(inputs=[inp], outputs=[out])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "pOoX_x9ewC7f",
    "outputId": "34ac52ea-8a1a-47e4-9225-b0be3bf98d20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 08:18:17.189273 139922671003520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "W0814 08:18:17.377123 139922671003520 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "W0814 08:18:18.215747 139922671003520 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "W0814 08:18:19.998931 139922671003520 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "W0814 08:18:20.172216 139922671003520 hdf5_format.py:221] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model_D\n",
      "Loading model_E\n",
      "Loading model_G\n",
      "Loading model_H\n"
     ]
    }
   ],
   "source": [
    "model_G = make_generator_resnet(\"G\")                   # First Generator\n",
    "model_H = make_generator_resnet(\"H\")                   # Second Generator\n",
    "model_D = make_discriminator(\"D\")                   # Discriminator for G\n",
    "model_E = make_discriminator(\"E\")                   # Discriminator for H\n",
    "\n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/parts/model_D_age.h5'):\n",
    "    print(\"Loading model_D\")\n",
    "    model_D.load_weights('/content/drive/My Drive/UTK_inthewild/parts/model_D_age.h5')\n",
    "    \n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/parts/model_E_age.h5'):\n",
    "    print(\"Loading model_E\")\n",
    "    model_E.load_weights('/content/drive/My Drive/UTK_inthewild/parts/model_E_age.h5')\n",
    "    \n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/parts/model_G_age.h5'):\n",
    "    print(\"Loading model_G\")\n",
    "    model_G.load_weights('/content/drive/My Drive/UTK_inthewild/parts/model_G_age.h5')\n",
    "    \n",
    "if os.path.exists('/content/drive/My Drive/UTK_inthewild/parts/model_H_age.h5'):\n",
    "    print(\"Loading model_H\")\n",
    "    model_H.load_weights('/content/drive/My Drive/UTK_inthewild/parts/model_H_age.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "IBx_cO4gxZW8",
    "outputId": "d405b392-e317-46cc-ddb7-180cdccdc11f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "model_7 (Model)              (None, 32, 32, 256)       379392    \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 32, 32, 256)       10630656  \n",
      "_________________________________________________________________\n",
      "model_8 (Model)              (None, 128, 128, 3)       378627    \n",
      "=================================================================\n",
      "Total params: 11,388,675\n",
      "Trainable params: 10,630,656\n",
      "Non-trainable params: 758,019\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fV9uoZN4xcSq",
    "outputId": "7291066b-e841-4e39-ae6d-8d33abc6d8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f41f66f0fd0>\n",
      "True\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f41f6706400>\n",
      "False\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f41f00b05f8>\n",
      "True\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f41f66b09b0>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for layer in model_G.layers:\n",
    "    print(layer)\n",
    "    print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uykLsaQTxm_a"
   },
   "outputs": [],
   "source": [
    "def get_image_list(path='images', young_lower=18, young_upper=30, old_lower=50, old_upper=80):\n",
    "    images = os.listdir(path)\n",
    "    ages = {\n",
    "        'young': [],\n",
    "        'old': []\n",
    "    }\n",
    "    \n",
    "    for image in images:\n",
    "        age = int(image.split('_')[0])\n",
    "        if young_lower <= age <= young_upper:\n",
    "            ages['young'].append(image)\n",
    "        if old_upper >= age >= old_lower:\n",
    "            ages['old'].append(image)\n",
    "        \n",
    "    return ages\n",
    "\n",
    "image_dataset = get_image_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTW0wYtIxqkS"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def generator(gen_func, *args):\n",
    "    while True:\n",
    "        yield gen_func(*args)\n",
    "        \n",
    "def create_young_old_batch(batch_size=32, shuffle_data=True):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    x = 128\n",
    "    images = get_image_list()['young']\n",
    "    to_create = np.random.choice(images, batch_size)\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (x, x))\n",
    "        X1.append(image)\n",
    "        \n",
    "    images = get_image_list()['old']\n",
    "    to_create = np.random.choice(images, batch_size)\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (x, x))\n",
    "        X2.append(image)\n",
    "        \n",
    "    X1 = np.stack(X1).astype(np.float32)\n",
    "    X2 = np.stack(X2).astype(np.float32)\n",
    "    \n",
    "    if shuffle_data:\n",
    "        X1, X2 = shuffle(X1, X2)\n",
    "        \n",
    "    return (X1.astype(np.float32) / 127.5) - 1.0, (X2.astype(np.float32) / 127.5) - 1.0\n",
    "\n",
    "def create_young_old_test_batch(batch_size=32, shuffle_data=True):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    size = 128\n",
    "    images = get_image_list()['young']\n",
    "    to_create = images[:batch_size]\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (size, size))\n",
    "        X1.append(image)\n",
    "        \n",
    "    images = get_image_list()['old']\n",
    "    to_create = images[:batch_size]\n",
    "    for img in to_create:\n",
    "        image = cv2.imread('images/' + img)\n",
    "        image = cv2.resize(image, (size, size))\n",
    "        X2.append(image)\n",
    "        \n",
    "    X1 = np.stack(X1).astype(np.float32)\n",
    "    X2 = np.stack(X2).astype(np.float32)\n",
    "    \n",
    "    if shuffle_data:\n",
    "        X1, X2 = shuffle(X1, X2)\n",
    "        \n",
    "    return (X1.astype(np.float32) / 127.5) - 1., (X2.astype(np.float32) / 127.5) - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5eX5UZyxsQD"
   },
   "outputs": [],
   "source": [
    "# disc_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# gen_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# cycled_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "identity_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = disc_loss_fn(tf.ones_like(real), real)\n",
    "    generated_loss = disc_loss_fn(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "def cycled_loss(real, cycled):\n",
    "    return cycled_loss_fn(real, cycled)\n",
    "\n",
    "def identity_loss(real, same):\n",
    "    return identity_loss_fn(real, same)\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return gen_loss_fn(tf.ones_like(generated), generated)\n",
    "\n",
    "@tf.function\n",
    "def train(genAB, genBA, discA, discB, X1, X2, lambda1=10.0):\n",
    "    real_A = X1\n",
    "    real_B = X2\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_B = genAB(real_A)                  # Young -> old\n",
    "        fake_A = genBA(real_B)                  # Old -> young\n",
    "        \n",
    "        cycled_A = genBA(fake_B)            # Young -> old -> young\n",
    "        cycled_B = genAB(fake_A)            # old -> young -> old\n",
    "        \n",
    "        same_B = genAB(real_B)                 # Old -> Old\n",
    "        same_A = genBA(real_A)                 # Young -> Young\n",
    "        \n",
    "        discA_fake_out = discA(fake_A)        # D predicts young, gen2, ie H, outputs young\n",
    "        discB_fake_out = discB(fake_B)\n",
    "\n",
    "        g_loss = mae_criterion(discA_fake_out, tf.ones_like(discA_fake_out)) \\\n",
    "                + mae_criterion(discB_fake_out, tf.ones_like(discB_fake_out)) \\\n",
    "                + lambda1 * abs_criterion(real_A, cycled_A) \\\n",
    "                + lambda1 * abs_criterion(real_B, cycled_B)  \n",
    "        \n",
    "        \n",
    "        discA_real_out = discA(real_A)\n",
    "        discB_real_out = discB(real_B)\n",
    "\n",
    "        discA_loss_real = mae_criterion(discA_real_out, tf.ones_like(discA_real_out))\n",
    "        discB_loss_real = mae_criterion(discB_real_out, tf.ones_like(discB_real_out))\n",
    "\n",
    "        discA_loss_fake = mae_criterion(discA_fake_out, tf.zeros_like(discA_fake_out))\n",
    "        discB_loss_fake = mae_criterion(discB_fake_out, tf.zeros_like(discB_fake_out))\n",
    "\n",
    "        discA_loss = (discA_loss_real + discA_loss_fake) / 2.0\n",
    "        discB_loss = (discB_loss_real + discB_loss_fake) / 2.0\n",
    "\n",
    "        d_loss = discA_loss + discB_loss\n",
    "                \n",
    "        net_loss = g_loss + d_loss\n",
    "           \n",
    "    G_vars = genAB.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, G_vars)\n",
    "    g1_optimizer.apply_gradients(zip(gradients, G_vars))\n",
    "    \n",
    "    H_vars = genBA.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, H_vars)\n",
    "    g2_optimizer.apply_gradients(zip(gradients, H_vars))\n",
    "    \n",
    "    D_vars = discA.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, D_vars)\n",
    "    d1_optimizer.apply_gradients(zip(gradients, D_vars))\n",
    "    \n",
    "    E_vars = discB.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, E_vars)\n",
    "    d2_optimizer.apply_gradients(zip(gradients, E_vars))\n",
    "    \n",
    "    del tape\n",
    "    gc.collect()\n",
    "    return net_loss, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-saH728xuFS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "fEYT1P67ySJf",
    "outputId": "aab8bec1-02e6-4e2e-8c71-24b308f95255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 10:52:36.889719 140622569072512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617/1618 [============================>.] - ETA: 1s - loss: 3.5127 - generator loss: 2.7448 - discriminator loss: 0.7679Epoch  1\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.9911 - generator loss: 2.4512 - discriminator loss: 0.5399Epoch  2\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.9972 - generator loss: 2.4593 - discriminator loss: 0.5380Epoch  3\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.8827 - generator loss: 2.4122 - discriminator loss: 0.4705Epoch  4\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 3.1559 - generator loss: 2.7018 - discriminator loss: 0.4540Epoch  5\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.9829 - generator loss: 2.3894 - discriminator loss: 0.5935Epoch  6\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.8415 - generator loss: 2.4351 - discriminator loss: 0.4064Epoch  7\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 3.0580 - generator loss: 2.6895 - discriminator loss: 0.3685Epoch  8\n",
      "1285/1618 [======================>.......] - ETA: 10:42 - loss: 3.1925 - generator loss: 2.9482 - discriminator loss: 0.2443"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/optimizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_young_old_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         progressbar.update(i, [\n\u001b[1;32m     16\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g1_optimizer = tf.keras.optimizers.Nadam(3e-5, beta_1=0.5)\n",
    "g2_optimizer = tf.keras.optimizers.Nadam(3e-5, beta_1=0.5)\n",
    "d1_optimizer = tf.keras.optimizers.Nadam(3e-4, beta_1=0.5)\n",
    "d2_optimizer = tf.keras.optimizers.Nadam(3e-4, beta_1=0.5)\n",
    "gc.collect()\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_D_age.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_E_age.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_G_age.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_H_age.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "qjLBExTJX--C",
    "outputId": "2547dba3-4834-48a3-f150-8c445fafae66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 01:19:57.244025 140068387350400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617/1618 [============================>.] - ETA: 2s - loss: 2.7280 - generator loss: 2.2775 - discriminator loss: 0.4505Epoch  1\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.6640 - generator loss: 2.2106 - discriminator loss: 0.4534Epoch  2\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.6176 - generator loss: 2.1658 - discriminator loss: 0.4518Epoch  3\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.5628 - generator loss: 2.1067 - discriminator loss: 0.4562Epoch  4\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.5257 - generator loss: 2.0683 - discriminator loss: 0.4574Epoch  5\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.4944 - generator loss: 2.0367 - discriminator loss: 0.4576Epoch  6\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.4654 - generator loss: 2.0075 - discriminator loss: 0.4579Epoch  7\n",
      " 896/1618 [===============>..............] - ETA: 23:35 - loss: 2.4449 - generator loss: 1.9881 - discriminator loss: 0.4568"
     ]
    }
   ],
   "source": [
    "g1_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "g2_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "d1_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)\n",
    "d2_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)\n",
    "gc.collect()\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_D_age.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_E_age.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_G_age.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_H_age.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "1VTK6O1XMwDL",
    "outputId": "f396cbf7-9530-439c-a7c8-5617933cf3a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 08:18:38.322208 139922671003520 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.4279 - generator loss: 1.9709 - discriminator loss: 0.4570Epoch  1\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.4063 - generator loss: 1.9484 - discriminator loss: 0.4580Epoch  2\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.3850 - generator loss: 1.9271 - discriminator loss: 0.4579Epoch  3\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.3678 - generator loss: 1.9096 - discriminator loss: 0.4582Epoch  4\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.3527 - generator loss: 1.8934 - discriminator loss: 0.4592Epoch  5\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.3375 - generator loss: 1.8777 - discriminator loss: 0.4598Epoch  6\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.3281 - generator loss: 1.8696 - discriminator loss: 0.4585Epoch  7\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.3138 - generator loss: 1.8542 - discriminator loss: 0.4595Epoch  8\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.3019 - generator loss: 1.8400 - discriminator loss: 0.4618Epoch  9\n",
      "1617/1618 [============================>.] - ETA: 1s - loss: 2.2866 - generator loss: 1.8256 - discriminator loss: 0.4610"
     ]
    }
   ],
   "source": [
    "g1_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "g2_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "d1_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)\n",
    "d2_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)\n",
    "gc.collect()\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_D_age.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_E_age.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_G_age.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_H_age.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eB4HGtwcBxt"
   },
   "outputs": [],
   "source": [
    "identity_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = disc_loss_fn(tf.ones_like(real), real)\n",
    "    generated_loss = disc_loss_fn(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "def cycled_loss(real, cycled):\n",
    "    return cycled_loss_fn(real, cycled)\n",
    "\n",
    "def identity_loss(real, same):\n",
    "    return identity_loss_fn(real, same)\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return gen_loss_fn(tf.ones_like(generated), generated)\n",
    "\n",
    "@tf.function\n",
    "def train(genAB, genBA, discA, discB, X1, X2, lambda1=10.0):\n",
    "    real_A = X1\n",
    "    real_B = X2\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_B = genAB(real_A)                  # Young -> old\n",
    "        fake_A = genBA(real_B)                  # Old -> young\n",
    "        \n",
    "        cycled_A = genBA(fake_B)            # Young -> old -> young\n",
    "        cycled_B = genAB(fake_A)            # old -> young -> old\n",
    "        \n",
    "        same_B = genAB(real_B)                 # Old -> Old\n",
    "        same_A = genBA(real_A)                 # Young -> Young\n",
    "        \n",
    "        discA_fake_out = discA(fake_A)        # D predicts young, gen2, ie H, outputs young\n",
    "        discB_fake_out = discB(fake_B)\n",
    "\n",
    "        g_loss = mae_criterion(discA_fake_out, tf.ones_like(discA_fake_out)) \\\n",
    "                + mae_criterion(discB_fake_out, tf.ones_like(discB_fake_out)) \\\n",
    "                + lambda1 * abs_criterion(real_A, cycled_A) \\\n",
    "                + lambda1 * abs_criterion(real_B, cycled_B)  \n",
    "        \n",
    "        \n",
    "        discA_real_out = discA(real_A)\n",
    "        discB_real_out = discB(real_B)\n",
    "\n",
    "        discA_loss_real = mae_criterion(discA_real_out, tf.ones_like(discA_real_out))\n",
    "        discB_loss_real = mae_criterion(discB_real_out, tf.ones_like(discB_real_out))\n",
    "\n",
    "        discA_loss_fake = mae_criterion(discA_fake_out, tf.zeros_like(discA_fake_out))\n",
    "        discB_loss_fake = mae_criterion(discB_fake_out, tf.zeros_like(discB_fake_out))\n",
    "\n",
    "        discA_loss = (discA_loss_real + discA_loss_fake) / 2.0\n",
    "        discB_loss = (discB_loss_real + discB_loss_fake) / 2.0\n",
    "\n",
    "        d_loss = discA_loss + discB_loss\n",
    "                \n",
    "        net_loss = g_loss + d_loss\n",
    "           \n",
    "    G_vars = genAB.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, G_vars)\n",
    "    g1_optimizer.apply_gradients(zip(gradients, G_vars))\n",
    "    \n",
    "    H_vars = genBA.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, H_vars)\n",
    "    g2_optimizer.apply_gradients(zip(gradients, H_vars))\n",
    "    \n",
    "    # D_vars = discA.trainable_variables\n",
    "    # gradients = tape.gradient(d_loss, D_vars)\n",
    "    # d1_optimizer.apply_gradients(zip(gradients, D_vars))\n",
    "    \n",
    "    # E_vars = discB.trainable_variables\n",
    "    # gradients = tape.gradient(d_loss, E_vars)\n",
    "    # d2_optimizer.apply_gradients(zip(gradients, E_vars))\n",
    "    \n",
    "    del tape\n",
    "    gc.collect()\n",
    "    return net_loss, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "J-YtVdYmcIu1",
    "outputId": "aa2a5283-9fad-4096-b821-31af3c3cbf79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      " 401/1618 [======>.......................] - ETA: 38:59 - loss: 3.1613 - generator loss: 2.6121 - discriminator loss: 0.5492"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/optimizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_young_old_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         progressbar.update(i, [\n\u001b[1;32m     12\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "for epoch in range(1):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_D_age.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_E_age.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_G_age.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_H_age.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFcEoGzleOJG"
   },
   "outputs": [],
   "source": [
    "identity_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = disc_loss_fn(tf.ones_like(real), real)\n",
    "    generated_loss = disc_loss_fn(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "def cycled_loss(real, cycled):\n",
    "    return cycled_loss_fn(real, cycled)\n",
    "\n",
    "def identity_loss(real, same):\n",
    "    return identity_loss_fn(real, same)\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return gen_loss_fn(tf.ones_like(generated), generated)\n",
    "\n",
    "@tf.function\n",
    "def train(genAB, genBA, discA, discB, X1, X2, lambda1=10.0):\n",
    "    real_A = X1\n",
    "    real_B = X2\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_B = genAB(real_A)                  # Young -> old\n",
    "        fake_A = genBA(real_B)                  # Old -> young\n",
    "        \n",
    "        cycled_A = genBA(fake_B)            # Young -> old -> young\n",
    "        cycled_B = genAB(fake_A)            # old -> young -> old\n",
    "        \n",
    "        same_B = genAB(real_B)                 # Old -> Old\n",
    "        same_A = genBA(real_A)                 # Young -> Young\n",
    "        \n",
    "        discA_fake_out = discA(fake_A)        # D predicts young, gen2, ie H, outputs young\n",
    "        discB_fake_out = discB(fake_B)\n",
    "\n",
    "        g_loss = mae_criterion(discA_fake_out, tf.ones_like(discA_fake_out)) \\\n",
    "                + mae_criterion(discB_fake_out, tf.ones_like(discB_fake_out)) \\\n",
    "                + lambda1 * abs_criterion(real_A, cycled_A) \\\n",
    "                + lambda1 * abs_criterion(real_B, cycled_B)  \n",
    "        \n",
    "        \n",
    "        discA_real_out = discA(real_A)\n",
    "        discB_real_out = discB(real_B)\n",
    "\n",
    "        discA_loss_real = mae_criterion(discA_real_out, tf.ones_like(discA_real_out))\n",
    "        discB_loss_real = mae_criterion(discB_real_out, tf.ones_like(discB_real_out))\n",
    "\n",
    "        discA_loss_fake = mae_criterion(discA_fake_out, tf.zeros_like(discA_fake_out))\n",
    "        discB_loss_fake = mae_criterion(discB_fake_out, tf.zeros_like(discB_fake_out))\n",
    "\n",
    "        discA_loss = (discA_loss_real + discA_loss_fake) / 2.0\n",
    "        discB_loss = (discB_loss_real + discB_loss_fake) / 2.0\n",
    "\n",
    "        d_loss = discA_loss + discB_loss\n",
    "                \n",
    "        net_loss = g_loss + d_loss\n",
    "           \n",
    "    G_vars = genAB.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, G_vars)\n",
    "    g1_optimizer.apply_gradients(zip(gradients, G_vars))\n",
    "    \n",
    "    H_vars = genBA.trainable_variables\n",
    "    gradients = tape.gradient(g_loss, H_vars)\n",
    "    g2_optimizer.apply_gradients(zip(gradients, H_vars))\n",
    "    \n",
    "    D_vars = discA.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, D_vars)\n",
    "    d1_optimizer.apply_gradients(zip(gradients, D_vars))\n",
    "    \n",
    "    E_vars = discB.trainable_variables\n",
    "    gradients = tape.gradient(d_loss, E_vars)\n",
    "    d2_optimizer.apply_gradients(zip(gradients, E_vars))\n",
    "    \n",
    "    del tape\n",
    "    gc.collect()\n",
    "    return net_loss, g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "t-VCxy7deSTp",
    "outputId": "629ee3e4-aa2d-4b1c-98ac-aa3e984919c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      " 233/1618 [===>..........................] - ETA: 55:32 - loss: 3.6735 - generator loss: 3.4848 - discriminator loss: 0.1887"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/optimizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_young_old_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         progressbar.update(i, [\n\u001b[1;32m     17\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g1_optimizer = tf.keras.optimizers.Nadam(3e-5, beta_1=0.5)\n",
    "g2_optimizer = tf.keras.optimizers.Nadam(3e-5, beta_1=0.5)\n",
    "d1_optimizer = tf.keras.optimizers.Nadam(3e-5, beta_1=0.5)\n",
    "d2_optimizer = tf.keras.optimizers.Nadam(3e-5, beta_1=0.5)\n",
    "\n",
    "gc.collect()\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(\"Epoch \", epoch)\n",
    "    batch_size=8\n",
    "    steps = int((len(image_dataset['young']) + len(image_dataset['old']))/batch_size)\n",
    "    progressbar = Progbar(steps)\n",
    "    for i in range(steps) :\n",
    "        X1, X2 = create_young_old_batch(batch_size=batch_size)\n",
    "        loss, gl, dl = train(model_G, model_H, model_D, model_E, X1, X2)\n",
    "        progressbar.update(i, [\n",
    "            ('loss', loss.numpy()),\n",
    "            ('generator loss', gl.numpy()),\n",
    "            ('discriminator loss', dl.numpy())\n",
    "        ])\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            os.mkdir(str(epoch) + '_' + str(i))\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/young2old')\n",
    "            os.mkdir(str(epoch) + '_' + str(i) + '/old2young')\n",
    "        \n",
    "            Xy, Xo = create_young_old_test_batch(batch_size=8)\n",
    "            Yy = np.around((1 + model_G.predict(Xy)) * 127.5)\n",
    "            Yo = np.around((1 + model_H.predict(Xo)) * 127.5)\n",
    "            Xy = np.around((1 + Xy) * 127.5)\n",
    "            Xo = np.around((1 + Xo) * 127.5)\n",
    "        \n",
    "            for j in range(len(Yy)):\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '.jpg', Yy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '.jpg', Yo[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/young2old/' + str(j) + '_actual.jpg', Xy[j])\n",
    "                cv2.imwrite(str(epoch) + '_' + str(i) + '/old2young/' + str(j) + '_actual.jpg', Xo[j])\n",
    "                            \n",
    "            model_D.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_D_age.h5')\n",
    "            model_E.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_E_age.h5')\n",
    "            model_G.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_G_age.h5')\n",
    "            model_H.save_weights('/content/drive/My Drive/UTK_inthewild/parts/model_H_age.h5')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZoVRHW-RTYM"
   },
   "outputs": [],
   "source": [
    "!rm -r '/content/drive/My Drive/UTK_inthewild/old2young'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLcdV0UKBxTc"
   },
   "outputs": [],
   "source": [
    "!mkdir '/content/drive/My Drive/UTK_inthewild/young2young'\n",
    "!mkdir '/content/drive/My Drive/UTK_inthewild/old2young'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "86Z-YsFtB_Qq",
    "outputId": "8df40e98-16c8-4280-b58c-1fe29476f5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting young2young\n",
      "starting old2young\n"
     ]
    }
   ],
   "source": [
    "print('starting young2young')\n",
    "for img in image_dataset['young']:\n",
    "    X = cv2.imread('images/'+img)\n",
    "    X = cv2.resize(X, (128, 128))\n",
    "    cv2.imwrite('/content/drive/My Drive/UTK_inthewild/young2young/'+img, X)\n",
    "    X = (X / 127.5) - 1.0\n",
    "    X = np.expand_dims(X, axis=0).astype(np.float32)\n",
    "    Y = np.squeeze(model_H.predict(X))\n",
    "    Y = (1.0 + Y) * 127.5\n",
    "    cv2.imwrite('/content/drive/My Drive/UTK_inthewild/young2young/'+img+'_young', Y)\n",
    "\n",
    "print('starting old2young')\n",
    "for img in image_dataset['old']:\n",
    "    X = cv2.imread('images/'+img)\n",
    "    X = cv2.resize(X, (128, 128))\n",
    "    try:\n",
    "        cv2.imwrite('/content/drive/My Drive/UTK_inthewild/old2young/'+img, X)\n",
    "    except:\n",
    "        continue\n",
    "    X = (X / 127.5) - 1.0\n",
    "    X = np.expand_dims(X, axis=0).astype(np.float32)\n",
    "    Y = np.squeeze(model_H.predict(X))\n",
    "    Y = (1.0 + Y) * 127.5\n",
    "    cv2.imwrite('/content/drive/My Drive/UTK_inthewild/old2young/'+img+'_young', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aAH6faNaKhFB",
    "outputId": "33fd2a84-cd35-4401-aff7-89a870761b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n"
     ]
    }
   ],
   "source": [
    "!tar czf young2old.tar '/content/drive/My Drive/UTK_inthewild/young2young'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B1qzwfImK9pJ",
    "outputId": "15491413-847a-4b57-d9e6-c770d169491d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n"
     ]
    }
   ],
   "source": [
    "!tar czf old2young.tar '/content/drive/My Drive/UTK_inthewild/old2young'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pDLbFA3WM0lh",
    "outputId": "ba1319fc-745d-4fd4-b3ea-3ab1bc8049b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8987\n"
     ]
    }
   ],
   "source": [
    "!ls -l '/content/drive/My Drive/UTK_inthewild/old2young' | wc -l"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CycleGAN_FaceAging_CommonEncDec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
